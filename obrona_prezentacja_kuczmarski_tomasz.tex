\documentclass[a4paper,9pt]{beamer}

\setbeamercovered{transparent}
%\usepackage{ifthen}
%\usepackage{caption}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}


%\captionsetup{font=scriptsize,labelfont=scriptsize}
\setbeamerfont{caption}{size=\scriptsize}


\setbeamertemplate{theorem begin}{{
\inserttheoremheadfont
\inserttheoremname
\inserttheoremnumber
\ifx \inserttheoremaddition \empty \else\ (\inserttheoremaddition)\fi%
\inserttheorempunctuation
\hspace{0.1cm}
}}
\setbeamertemplate{theorem end}{}

\newtheoremstyle{mytheoremstyle} % name
        {\topsep}                    % Space above
        {\topsep}                    % Space below
        {\itshape\fontfamily{ptm}\selectfont}                   % Body font
        {}                           % Indent amount
        {\fontfamily{ptm}\selectfont\scshape\color{blue}}                   % Theorem head font
        {:}                          % Punctuation after theorem head
        {.5em}                       % Space after theorem head
        {}  % Theorem head spec (can be left empty, meaning ‘normal’)
\theoremstyle{mytheoremstyle}


\newtheorem{hypothesis}{Hypothesis}
\newtheorem*{unnumberedhypothesis}{Hypothesis}
\newtheorem*{unnumberednullhypothesis}{Null Hypothesis}
\newtheorem{notion}{Notion}

%\usepackage{polski}
%\usepackage[cp1250]{inputenc}
\usepackage{tcolorbox}
\usepackage{tikz}


\usetheme{Warsaw}
\setbeamercovered{transparent}


%\logo{\includegraphics[width=.1\textwidth]{res/logo_uam}}
\begin{document}

\title[Modeling of Polish Intonation for SPSS]{Modeling of Polish Intonation for Statistical-Parametric Speech Synthesis}
\author[Tomasz Kuczmarski]{Tomasz Kuczmarski\\
\vspace{0.5cm}
\small{Prof. zw. Dr hab. In\.z. Gra\.zyna Demenko}\\
\textit{\tiny{Supervisor}}}
\institute{\protect{\small{Adam Mickiewicz University}}\\
\includegraphics[width=.1\textwidth]{res/logo_uam}\\
\tiny{Faculty of Modern Languages and Literature}\\
\tiny{Institute of Ethnolinguistics}\\
}

% WYWALIĆ
\date{\tiny{May 18, 2022}}

%
\begin{frame}
\titlepage
\end{frame}




\begin{frame}
\frametitle{Intonation}
\framesubtitle{Definition}
\vspace{2cm}
\textit<1-2>{``The tonal structure of speech expressed by the melody produced by our larynx. It has a phonetic aspect, the fundamental frequency ($F_{0}$), and a grammatical (phonological) aspect.''} (F\'ery 2016)
\begin{columns}
\column{0.5\textwidth}
\column{0.5\textwidth}
\vspace{0.5cm}
\invisible<1>{\begin{alertblock}{}
All definitions of intonation "are epistemological
definitions,i.e., not a priori programmatic definitions, but a posteriori statements of a practice and methodology." (Rossi 2000)
\end{alertblock}}
\end{columns}
\end{frame}

\section{Motivation}

\begin{frame}
\frametitle{Motivation}
\begin{columns}
\column{0.5\textwidth}
\begin{center}
\includegraphics[width=\textwidth]{res/mapping_highlight.png}
\end{center}
\column{0.5\textwidth}
\scriptsize{
\begin{exampleblock}{Motivation}
\begin{itemize}
\item[\checkmark] Dualistic gap between phonology and phonetics.
\item[\checkmark] Unification within a broader metatheory.
\item[\checkmark] Unknown nature of the mappings between mental categories and continuous contours of $F_{0}$.
\item[\checkmark] How linguistic features of an utterance influence its $F_{0}$ contours.
\item[\checkmark] Need for a physicalist (neurobiological) model.
\item[\checkmark] Modern statstical-parametric speech synthesis provides a framework for experimentation and evaluation of such
\item[\checkmark] Remarkable properties of biologically-motivated Convolutional Neural Networks.
\end{itemize}
\end{exampleblock}
}
\end{columns}
\end{frame}

\section{Objectives}
\subsection{Objectives}
\begin{frame}
\frametitle{Objectives}
\begin{enumerate}
\item<1-2> Build a robust biologically-inspired neural model of the probabilistic mapping between discrete low-level linguistic features of an utterance and its intonation contours ($F_{0}$ values).
\item<2> Build a state-of-the-art neural source-filter resynthesis framework for Polish read speech.
\item<2> Deploy the intonation model within the resynthesis framework and measure the perceived naturalness of the output intonation contours, and to
\item<2> operationalize the results of these measurements as an indicator of the model's robustness.
\item<2> Develop a method to explain the relevance of the individual input linguistic features for the produced intonation contour.
\item<1-2> Analyze how specific linguistic features contribute to the $F_{0}$ contours of an utterance.
\end{enumerate}
\end{frame}

\subsection{Hypotheses}
\begin{frame}
\frametitle{Main Hypotheses}
\begin{tcolorbox}
\begin{hypothesis}[] The continous $F_0$ contours of an utterance emerge from its discrete linguistic features through a series of successive probabilistic mappings into intermediate latent represenentations.
\end{hypothesis}
\begin{hypothesis}
The biologically-inspired Deep Temporal Convolutional Network can be an effective model of these mappings and hence of Polish neutral read speech intonation in the context of statistical-parametric speech synthesis. \label{secondary:a}
\end{hypothesis}
\begin{hypothesis}
The set of shallow linguistic features used in this thesis provides information which is sufficient for synthesis of natural sounding intonation in the context of statistical-parametric speech synthesis. \label{secondary:b}
\end{hypothesis}
\end{tcolorbox}
\end{frame}

\subsection{Contributory Methodological Hypothesis}
\begin{frame}
\frametitle{Contributory Methodological Hypothesis}
\begin{tcolorbox}
\begin{hypothesis}[contributory methodological]
\label{contributory}
A Deep Temporal Convolutional Network can become an explanatory scientific model of mappings between linguistics features and the intonation of an utterance.
\end{hypothesis}
\end{tcolorbox}
\end{frame}

\section{Background}
%2. Stan badan na swiecie i w Polsce (tu trzeba wspomniec i Gibona i Moebiusa)
%Odnosnie modelowania intonacji prosze nie popelnic bledu jak na egzaminie, pani Klessa nie zajmowala sie nigdy modelowaniem intonacji!
%Natomiast duzo pracy w modelowanie intonacji polskiej wlozyla Agnieszka na podstawie mojej pracy
%-Modelowanie intonacji na potrzeby technologii mowy”

% TODO: Dodać notke o brakach Fujisaki, Bollinger np. z cytatem o gigancie + cognitive & biolinguistic revolution, Skinner etc.

% NOTES: Acoustic correlates rather than phonetic models

\subsection{Intonation research in the world}
\begin{frame}
\frametitle{Background}
\includegraphics[width=\textwidth]{res/background.png}
\end{frame}

\subsection{Intonation research in Poland}
\begin{frame}
\frametitle{Background}
\includegraphics[width=\textwidth]{res/intonation_background_pl.png}
\end{frame}

\subsection{Speech synthesis}
\begin{frame}
\frametitle{Speech synthesis}
\begin{columns}
\begin{column}{0.6\textwidth}
\includegraphics[width=\textwidth]{res/speech_synthesis_clunit_hmm.png}
\end{column}
\vrule{}
\column{0.4\textwidth}
\vspace{2cm}
\includegraphics[width=\textwidth]{res/speech_synthesis_wavenet.png}\\
\end{columns}
\end{frame}

\begin{frame}
\frametitle{Speech synthesis - Wavenet}
\begin{columns}
\column{0.4\textwidth}
\begin{figure}
\begin{center}
  \includegraphics[width=\textwidth]{res/wavenet_evaluation.png}
\end{center}
	\caption{Google WaveNet evaluation results. (from van den Oord 2016).}
\end{figure}
\begin{exampleblock}{}
\scriptsize{
Wavenet belongs to a class of models known as
\textbf{Convolutional Neural Networks} (CNNs)
which are used mainly in the area of of image recognition where they excel.}
\end{exampleblock}
\begin{center}
  \includegraphics[width=0.5cm]{res/speaker_icon.png}
\end{center}
\column{0.6\textwidth}
\includegraphics[width=\textwidth]{res/tcn.png}
\end{columns}
\end{frame}

\subsection{Neurobiological foundations of CNNs}

\begin{frame}
\frametitle{Visual processing in the human brain}
\begin{figure}
\begin{center}
  \includegraphics[width=0.75\textwidth]{res/human_vision.png}
\end{center}
	\caption{Hierarchical, feedforward visual processing in human brain. (Adopted from Manassi et al. 2013)}
\end{figure}
\end{frame}

\begin{frame}
\frametitle{CNN feature maps}
\begin{figure}
\begin{center}
  \includegraphics[width=\textwidth]{res/lenet.png}
\end{center}
	\caption{Image recognition Convolutional Neural Network (LeNet-5). (Adopted from LeCun et al. 1989)}
\end{figure}
\begin{center}
\includegraphics[width=0.6\textwidth]{res/filter_vis.png}
\end{center}
\end{frame}

\begin{frame}
\frametitle{Simple and complex cells in the visual cortex}
\begin{columns}
\column{0.5\textwidth}
\begin{figure}
\begin{center}
  \includegraphics[width=0.75\textwidth]{res/hubel-experiment-cat.png}
\end{center}
	\caption{Famous Hubel and Wiesel cat experiment. (adopted from Hubel and Wiesel 1959).}
\end{figure}
\column{0.5\textwidth}
\vspace{0.8cm}
\begin{figure}
\begin{center}
  \includegraphics[width=0.6\textwidth]{res/hubel-experiment-cat-2.png}
\end{center}
	\caption{Neural response of simple cells. (adopted from Hubel and Wiesel 1968).}
\end{figure}
\end{columns}
\hrule
\begin{columns}
\column{0.5\textwidth}
\begin{figure}
\begin{center}
  \includegraphics[width=0.6\textwidth]{res/simple_cells.png}
\end{center}
	\caption{Simple receptive fields. (adopted from Hubel and Wiesel 1962).}
\end{figure}
\column{0.5\textwidth}
\vspace{0.2cm}
\begin{figure}
\begin{center}
  \includegraphics[width=0.75\textwidth]{res/complex_cells.png}
\end{center}
	\caption{Three different types of complex receptive fields. (adopted from Hubel and Wiesel 1962).}
\end{figure}
\end{columns}
\end{frame}

\begin{frame}
\frametitle{Neurobiological foundations of CNNs}
\begin{columns}
\column{0.5\textwidth}
\begin{figure}
\begin{center}
  \includegraphics[width=0.75\textwidth]{res/convolution.png}
\end{center}
	\caption{Example of a 2-dimensional matrix convolution.}
\end{figure}
\column{0.5\textwidth}
\begin{figure}
\begin{center}
  \includegraphics[width=0.75\textwidth]{res/convolution_kernels_examples.png}
\end{center}
	\caption{Examples of convoluting and image with different convolution kernels. (Adopted from the Wikipedia).}
\end{figure}
\end{columns}
\hrule
\begin{columns}
\column{0.5\textwidth}
\begin{figure}
\begin{center}
  \includegraphics[width=0.75\textwidth]{res/simple_cells.png}
\end{center}
	\caption{Simple receptive fields. (adopted from Hubel and Wiesel 1962).}
\end{figure}
\column{0.5\textwidth}
\vspace{0.7cm}
\scriptsize{
\begin{exampleblock}{}
\textbf{Simple cells} perform edge and line detection which can be very effectively approximated with matrix \textbf{convolution}.
\end{exampleblock}
}
\end{columns}
\end{frame}


\begin{frame}
\frametitle{Neurobiological foundations of CNNs}
\begin{columns}
\column{0.5\textwidth}
\scriptsize{
\begin{exampleblock}{}
The computations that the \textbf{complex cells} perform are similiar to the \textbf{max pooling} operation.
\end{exampleblock}
}
\column{0.5\textwidth}
\begin{figure}
\begin{center}
  \includegraphics[width=0.75\textwidth]{res/max_pooling.png}
\end{center}
	\caption{Example of a 2x2 max pooling matrix operation.}
\end{figure}
\hrule
\begin{figure}
\begin{center}
  \includegraphics[width=0.75\textwidth]{res/complex_cells.png}
\end{center}
	\caption{Three different types of complex receptive fields. (adopted from Hubel and Wiesel 1962).}
\end{figure}
\end{columns}
\end{frame}

\begin{frame}
\frametitle{Simple and complex cells in the auditory cortex}
\begin{figure}
\begin{center}
  \includegraphics[width=0.9\textwidth]{res/simple_cells_auditory_cortes.png}
\end{center}
	\caption{Responses of single neurons in primary auditory cortex (A1) of rhesus monkeys to band-passed noise (BPN) bursts centered at particular frequencies.. (Adopted from Tian et al. 2013)}
\end{figure}
\begin{columns}
\column{0.3\textwidth}
\column{0.7\textwidth}
\scriptsize{
\begin{exampleblock}{}
Auditory cortex also contains \textbf{simple (and complex) cells} with two-dimensional receptive fields that are similar to those in the visual cortex but which operate in the \textbf{spectrotemporal domain} instead.
\end{exampleblock}
}
\end{columns}
\end{frame}

\subsection{Explanatory properties of CNNs}

\begin{frame}
\frametitle{Explanatory properties of CNNs}
\begin{figure}
\begin{center}
  \includegraphics[width=0.65\textwidth]{res/sensitivity_vs_relevance}
\end{center}
	\caption{Results of sensitivity-based and relevance-based explainability methods. (Based on Samek 2016)}
\end{figure}
\end{frame}


\section{Methods}

\subsection{Dataset}
%4.Material eksperymentalny


\begin{frame}
\frametitle{Dataset}
\begin{figure}
\begin{center}
  \includegraphics[width=0.8\textwidth]{res/dataset_blf}
\end{center}
	\caption{Speech corpus built originally for the purpose of the Polish BOSS unit selection synthesizer (Demenko, Bachan, M{\"o}bius 2008; Demenko, Klessa, Szyma\'nski, Breuer, Hess 2010).}
\end{figure}
\end{frame}


\subsection{Data preprocessing}

\begin{frame}
\frametitle{Data preprocessing}
\begin{center}
  \includegraphics[width=0.9\textwidth]{res/dataset_preprocessing}
\end{center}
\end{frame}


\subsection{Feature extraction}

\begin{frame}
\frametitle{Feature extraction}
\begin{center}
  \includegraphics[width=\textwidth]{res/dataset_feature_extraction}
\end{center}
\end{frame}



\subsection{Model implementation}

\begin{frame}
\frametitle{Model implementation}
\begin{columns}
\column{0.6\textwidth}
\begin{center}
  \includegraphics[width=\textwidth]{res/tcn}
\end{center}
\column{0.4\textwidth}
\scriptsize{
\begin{exampleblock}{TCN parameters}
\begin{itemize}
\item 6 residual layers,
\item 64 convolution filters of length 2
\item $2^n$ dilations ($[1, 2, 4, 8, 16, 32]$)
\item 2 final fully connected dense layers (ReLU activation; sizes 64 and 1)
\end{itemize}
\end{exampleblock}
\begin{exampleblock}{Complete code repository}
\begin{itemize}
\item[\checkmark] \url{https://github.com/mrslacklines/intonation_synthesis}
\end{itemize}
\end{exampleblock}
\begin{block}
{}
\scriptsize{\url{github.com/philipperemy/keras-tcn}}
\end{block}
}
\end{columns}
\end{frame}

\begin{frame}
\frametitle{Model training infrastructure}
\begin{columns}
\begin{column}{0.7\textwidth}
\begin{center}
  \includegraphics[width=\textwidth]{res/cloud_training}
\end{center}
\hrule
\begin{itemize}
\item 64 CPUs; 488GB
\item 8 NVIDIA Tesla V100 GPUs; 128GB
\end{itemize}
\scriptsize{
\begin{exampleblock}{Memory requirements}
A single batch of data, which is a $64\times1900\times1297$ vector of 8-byte boolean
values, occupies 1.2617216 gigabytes of memory, and the model comprises of a total
of 449,409 parameters (446,337 trainable and 3,072 non-trainable).
\end{exampleblock}
}
\end{column}
\vrule{}
\column{0.3\textwidth}
\begin{center}
\includegraphics[width=2cm]{res/implementation_framework}
\end{center}
\hrule
\vspace{0.1cm}
\scriptsize{
Training parameters:
\begin{itemize}
\item ADAM optimizer
\item initial learning rate of $0.1$,
\item $\beta_{1} = 0.9$,
\item $\beta_{2} = 0.999$,
\item $\varepsilon = 1e-07$.
\item \textbf{Loss metric}: Mean Squared Error (MSE)
\item 200 epochs
\item random 8:1:1 dataset split
\end{itemize}
}
\end{columns}
\end{frame}

\subsection{Inference and relevance analysis}

\begin{frame}
\frametitle{$F_{0}$ inference and feature relevance analysis}
\begin{columns}
\column{0.3\textwidth}
\begin{figure}
\begin{center}
  \includegraphics[width=0.9\textwidth]{res/deep_taylor}
\end{center}
	\caption{Computational flow of deep Taylor decomposition. (Adopted from Montavon 2017).}
\end{figure}
\scriptsize{
\begin{exampleblock}{}
{INNvestigate} Neural Networks!\\(Alber et al. 2019)\\
\vspace{1cm}
\url{github.com/albermax/innvestigate}
\vspace{0.2cm}
\end{exampleblock}
}
\column{0.7\textwidth}
\includegraphics[width=\textwidth]{res/inference_and_relevance_analysis}
\end{columns}
\end{frame}

\subsection{Evaluation}

\begin{frame}
\frametitle{Resynthesis}
\begin{center}
  \includegraphics[width=\textwidth]{res/resynthesis}
\end{center}
\scriptsize{
\begin{block}{}
Neural Source Filter Vocoder from (X. Wang et al. 2019)
\end{block}
}
\end{frame}

\begin{frame}
\frametitle{Perceptual evaluation experiment}
\begin{columns}
\begin{column}{0.5\textwidth}
\begin{center}
  \includegraphics[width=\textwidth]{res/experiment_infra}
\end{center}
  \includegraphics[width=0.5\textwidth]{res/experiment_stack}
\end{column}
\column{0.5\textwidth}
  \includegraphics[width=0.8\textwidth]{res/experiment_diagram}
  \scriptsize{
\begin{exampleblock}{}
\textbf{Available at:}\\
\url{fonetyka.cudaniewidy.org/experiment}\\
\textbf{Code at:}\\
\url{github.com/mrslacklines/listening_experiments}
\end{exampleblock}
}
\end{columns}
\end{frame}


\section{Results}
%5. Wyniki

\begin{frame}
\frametitle{$F_{0}$ inference results}
\begin{figure}
\begin{center}
  \includegraphics[width=0.7\textwidth]{res/amu_pl_ilo_BAZA_2006A_zbitki_A0025_simple_pred_freq}
\end{center}
	\caption{Result of $F_0$ prediction for "Lokatorzy znale\'zli si\k{e} w podbramkowej sytuacji i musieli si\k{e} wyprowadzi\'c" (\textit{The tenants found themselves in a difficult situation and had to move out}).}
\end{figure}
\end{frame}

\begin{frame}
\frametitle{$F_{0}$ inference results}
\begin{figure}
\begin{center}
  \includegraphics[width=0.7\textwidth]{res/amu_pl_ilo_BAZA_2006C_C0545_simple_pred_freq}
\end{center}
	\caption{Result of $F_0$ prediction for "Powodzenie nie jest gwarantowane" (\textit{Success is not guaranteed}).}
\end{figure}
\end{frame}

\begin{frame}
\frametitle{$F_{0}$ inference results}
\begin{figure}
\begin{center}
  \includegraphics[width=0.7\textwidth]{res/amu_pl_ilo_BAZA_2006C_C0221_simple_pred_freq}
\end{center}
	\caption{Result of $F_{0}$ prediction for "Gadu\l{}a by\l{}a bardzo niezno\'sna" (\textit{Gabby was very annoying.}).}
\end{figure}
\end{frame}

\begin{frame}
\frametitle{$F_{0}$ inference results}
\begin{figure}
\begin{center}
  \includegraphics[width=0.7\textwidth]{res/amu_pl_ilo_BAZA_2006C_C0271_simple_pred_freq}
\end{center}
	\caption{Result of $F_0$ prediction for "Mo\.ze przynios\k{a} te\.z gorza\l{}\k{e}" (\textit{Maybe they will bring booze too.}).}
\end{figure}
\end{frame}

\begin{frame}
\frametitle{$F_{0}$ inference results}
\begin{figure}
\begin{center}
  \includegraphics[width=0.7\textwidth]{res/amu_pl_ilo_BAZA_2006C_C0278_simple_pred_freq}
\end{center}
	\caption{Result of $F_0$ prediction for "To jest wa\.zna godzina dla nas wszystkich" (\textit{This is an important hour for all of us}).}
\end{figure}
\end{frame}

\begin{frame}
\frametitle{$F_{0}$ inference results}
\begin{figure}
\begin{center}
  \includegraphics[width=0.7\textwidth]{res/amu_pl_ilo_BAZA_2006C_C0362_simple_pred_freq}
\end{center}
	\caption{Result of $F_0$ prediction for "My\'sl\k{e}, \.ze chleb razowy b\k{e}dzie najlepszy" (\textit{I think that a wholemeal bread will be the best}).}
\end{figure}
\end{frame}

\begin{frame}
\frametitle{$F_{0}$ inference results}
\begin{figure}
\begin{center}
  \includegraphics[width=0.7\textwidth]{res/amu_pl_ilo_BAZA_2006A_zbitki_A0036_simple_pred_freq}
\end{center}
	\caption{Result of $F_0$ prediction for "S\l{}ysza\l{}am odg\l{}os zbli\.zaj\k{a}cego si\k{e} poci\k{a}gu" (\textit{I heard the sound of an approaching train}).}
\end{figure}
\end{frame}

\begin{frame}
\frametitle{$F_{0}$ inference results}
\begin{figure}
\begin{center}
  \includegraphics[width=0.7\textwidth]{res/amu_pl_ilo_BAZA_2006C_C0160_simple_pred_freq}
\end{center}
	\caption{Result of $F_0$ prediction for "Czy to by\l{} \l{}atwy dob\'or?" (\textit{Was it an easy choice?}).}
\end{figure}
\end{frame}

\begin{frame}
\frametitle{$F_{0}$ inference results}
\begin{figure}
\begin{center}
  \includegraphics[width=0.7\textwidth]{res/amu_pl_ilo_BAZA_2006D_D1140_simple_pred_freq}
\end{center}
	\caption{Result of $F_0$ prediction for "To Majka" (\textit{This is Majka}).}
\end{figure}
\end{frame}

\begin{frame}
\frametitle{$F_{0}$ inference results}
\begin{figure}
\begin{center}
  \includegraphics[width=0.7\textwidth]{res/amu_pl_ilo_BAZA_2006C_C0208_simple_pred_freq}
\end{center}
	\caption{Result of $F_0$ prediction for "Na czym polega kandyzacja?" (\textit{How does candying work?}).}
\end{figure}
\end{frame}

%\begin{frame}
%\frametitle{Objective evaluation results}
%\begin{columns}
%\column{0.5\textwidth}
%\vspace{1.3cm}
%\begin{figure}
%\begin{center}
%  \includegraphics[width=0.9\textwidth]{res/mse}
%\end{center}
%	\caption{Mean MSE scores in the test dataset for $\log F_0$ predictions.}
%\end{figure}
%\column{0.5\textwidth}
%\includegraphics[width=0.9\textwidth]{res/mse_formula}
%\end{columns}
%\end{frame}


\begin{frame}
\frametitle{Subjective evaluation results - MOS}
\begin{figure}
\begin{center}
  \includegraphics[width=0.35\textwidth]{res/MOS-answer}
\end{center}
	\caption{Mean Opinion Score-based evaluation results.}
\end{figure}
\begin{columns}
\column{0.3\textwidth}
\begin{figure}
\begin{center}
\includegraphics[width=0.5\textwidth]{res/MOS_number_of_scores_natural_stimuli}
\end{center}
\caption{Mean Opinion Score-based evaluation total numbers of specific scores for natural stimuli.}
\end{figure}
\column{0.3\textwidth}
\begin{figure}
\begin{center}
  \includegraphics[width=0.5\textwidth]{res/MOS_stimulus_type_mean_response_time}
\end{center}
	\caption{Mean Opinion Score-based evaluation mean response times for synthetic and natural stimuli.}
\end{figure}
\column{0.3\textwidth}
\begin{figure}
\begin{center}
\includegraphics[width=0.5\textwidth]{res/MOS_number_of_scores_synthetic_stimuli}
\end{center}
\caption{Mean Opinion Score-based evaluation total numbers of specific scores for synthetic stimuli.}
\end{figure}
\end{columns}
\end{frame}

\begin{frame}
\frametitle{Subjective evaluation results - MOS}
\begin{figure}
\begin{center}
\includegraphics[width=0.5\textwidth]{res/MOS_correlation_matrix}
\end{center}
\caption{Mean Opinion Score-based evaluation parameters correlation matrix.}
\end{figure}
\end{frame}


\begin{frame}
\frametitle{Subjective evaluation results -- ABX}
%\scriptsize{
\begin{exampleblock}{$\tilde{\chi}^2$ test}
\begin{unnumberednullhypothesis}[$H_0$]
	There are no perceptually significant differences between resynthesized recordings with synthetic and natural $F_0$.
\end{unnumberednullhypothesis}
\begin{unnumberedhypothesis}[$H_a$]
	There are perceptually significant differences between resynthesized recordings with synthetic and natural $F_0$.
\end{unnumberedhypothesis}
\end{exampleblock}
%}
\begin{columns}
\column{0.5\textwidth}
\begin{center}
  \includegraphics[width=\textwidth]{res/ABX_individual}
\end{center}
\column{0.5\textwidth}
\begin{center}
  \includegraphics[width=\textwidth]{res/ABX_histogram}
\end{center}
\end{columns}
\end{frame}


\begin{frame}
\frametitle{Subjective evaluation results -- ABX}
\begin{columns}
\column{0.4\textwidth}
\begin{figure}
\begin{center}
  \includegraphics[width=\textwidth]{res/ABX_stimulus_type_correct_answers}
\end{center}
	\caption{ABX experiment number of correct and incorrect answers in case of natural and synthetic stimuli.}
\end{figure}
\column{0.6\textwidth}
\begin{figure}
\begin{center}
  \includegraphics[width=0.3\textwidth]{res/ABX_stimulus_type_mean_response_time}
\end{center}
	\caption{ABX experiment mean response times for synthetic and natural stimuli.}
\end{figure}
\begin{figure}
\begin{center}
  \includegraphics[width=0.6\textwidth]{res/ABX_correlation_matrix}
\end{center}
	\caption{ABX experiment parameters correlation matrix.}
\end{figure}
\end{columns}
\end{frame}


\begin{frame}
\frametitle{Feature relevance analysis results}
\begin{figure}
\begin{center}
  \includegraphics[width=\textwidth]{res/amu_pl_ilo_baza_2006a_zbitki_a0036_zoom}
\end{center}
	\caption{Fundamental frequency predictions for "S\l{}ysza\l{}am odg\l{}os zbli\.zaj\k{a}cego si\k{e} poci\k{a}gu" (\textit{I heard the sound of an approaching train}) aligned with feature relevance heatmap.}
\end{figure}
\end{frame}

\begin{frame}
\frametitle{Feature relevance analysis results}
\begin{columns}
\column{0.2\textwidth}
\begin{figure}
	\caption{Fundamental frequency predictions for "S\l{}ysza\l{}am odg\l{}os zbli\.zaj\k{a}cego si\k{e} poci\k{a}gu" (\textit{I heard the sound of an approaching train}) aligned with relevance heatmap for general feature groups.}
\end{figure}
\column{0.8\textwidth}
\begin{center}
  \includegraphics[width=\textwidth]{res/general_feature_categories_amu_pl_ilo_baza_2006a_zbitki_a0036}
\end{center}
\end{columns}
\end{frame}

\begin{frame}
\frametitle{Feature relevance analysis results}
\begin{columns}
\column{0.2\textwidth}
\begin{figure}
	\caption{Fundamental frequency predictions for "Waluta jeny - w j\k{e}zyku greckim - jest cenna" (\textit{The currency yen, in Greek, is valuable}) aligned with relevance heatmap for general feature groups.}
\end{figure}
\column{0.8\textwidth}
\begin{center}
  \includegraphics[width=\textwidth]{res/general_feature_categories_amu_pl_ilo_baza_2006d_d0100}
\end{center}
\end{columns}
\end{frame}

\begin{frame}
\frametitle{Feature relevance analysis results}
\begin{columns}
\column{0.3\textwidth}
\begin{figure}
	\caption{Fundamental frequency predictions for "Musisz dojrze\'c do tego, by to zrozumie\'c" (\textit{You have to grow up to understand it}) aligned with relevance heatmap for features grouped by the level of utterance.}
\end{figure}
\column{0.7\textwidth}
\begin{center}
  \includegraphics[width=\textwidth]{res/linguistic_levels_amu_pl_ilo_baza_2006e_e1014}
\end{center}
\end{columns}
\end{frame}

\begin{frame}
\frametitle{Feature relevance analysis results}
\begin{columns}
\column{0.3\textwidth}
\begin{figure}
	\caption{Fundamental frequency predictions for "Przep\l{}yne\l{}am na grzbiecie siedemna\'scie d\l{}ugo\'sci basenu" (\textit{I swam seventeen pool lengths on my back.}) aligned with relevance heatmap for features grouped by the type of relation.}
\end{figure}
\column{0.7\textwidth}
\begin{center}
  \includegraphics[width=\textwidth]{res/relation_types_amu_pl_ilo_baza_2006a_zbitki_a0135}
\end{center}
\end{columns}
\end{frame}


\begin{frame}
\frametitle{Feature relevance analysis results}
\begin{columns}
\column{0.3\textwidth}
\begin{figure}
	\caption{Fundamental frequency predictions for "Ciocia pracuje w urz\k{e}dzie pa\'nstwowym" (\textit{Aunt works in a government office}) aligned with relevance heatmap for features grouped by the type of relation and the level of utterance.}
\end{figure}
\column{0.7\textwidth}
\begin{center}
  \includegraphics[width=\textwidth]{res/linguistic_levels_and_relation_types_amu_pl_ilo_baza_2006a_zbitki_a0139}
\end{center}
\end{columns}
\end{frame}


\begin{frame}
\frametitle{Feature relevance analysis results}
\begin{center}
  \includegraphics[width=0.8\textwidth]{res/cumulative_relevance.png}
\end{center}
\end{frame}


\begin{frame}
\frametitle{Feature relevance analysis results}
\begin{columns}
\column{0.3\textwidth}
\begin{figure}
	\caption{Regular mean relevance per feature group. The $y$-axis scaling due to the high relevance of V/UV renders comparatively flat plots for other features.}
\end{figure}
\column{0.55\textwidth}
\begin{center}
  \includegraphics[width=\textwidth]{res/feature_relevance_ranking_-_mean_(sum)_-_general_feature_categories}
\end{center}
\end{columns}
\end{frame}


\begin{frame}
\frametitle{Feature relevance analysis results}
\begin{columns}
\column{0.3\textwidth}
\begin{figure}
	\caption{Regular mean relevance per feature group with the most relevant feature (V/UV) excluded.}
\end{figure}
\column{0.55\textwidth}
\begin{center}
  \includegraphics[width=\textwidth]{res/feature_relevance_ranking_-_mean_(sum)_-_general_feature_categories_-_no_vuv}
\end{center}
\end{columns}
\end{frame}

\begin{frame}
\frametitle{Feature relevance analysis results}
\begin{columns}
\column{0.3\textwidth}
\begin{figure}
	\caption{Regular mean relevance per feature group with the most relevant feature (V/UV) excluded. Medium granularity of feature groups.}
\end{figure}
\column{0.7\textwidth}
\begin{center}
  \includegraphics[width=0.8\textwidth]{res/feature_relevance_ranking_-_mean_(sum)_-_detailed_feature_categories_-_no_vuv}
\end{center}
\end{columns}
\end{frame}

\begin{frame}
\frametitle{Feature relevance analysis results}
\begin{figure}
\begin{center}
  \includegraphics[width=\textwidth]{res/feature_relevance_ranking_-_mean_(sum)_-_all_feature_categories_-_no_vuv}
\end{center}
	\caption{Regular mean relevance per feature group with the most relevant feature (V/UV) excluded. High granularity of feature groups.}
\end{figure}
\end{frame}

\begin{frame}
\frametitle{Feature relevance analysis results}
\begin{columns}
\column{0.3\textwidth}
\begin{figure}
	\caption{Absolute mean relevance per feature group with the most relevant feature (V/UV) excluded.}
\end{figure}
\column{0.55\textwidth}
\begin{center}
  \includegraphics[width=\textwidth]{res/feature_relevance_ranking_-_mean_(absolute_sum)_-_general_feature_categories_-_no_vuv}
\end{center}
\end{columns}
\end{frame}

\begin{frame}
\frametitle{Feature relevance analysis results}
\begin{columns}
\column{0.3\textwidth}
\begin{figure}
	\caption{Positive relevance-only mean per feature group with the most relevant feature (V/UV) excluded.}
\end{figure}
\column{0.55\textwidth}
\begin{center}
  \includegraphics[width=\textwidth]{res/feature_relevance_ranking_-_mean_(positive_only_sum)_-_general_feature_categories_-_no_vuv}
\end{center}
\end{columns}
\end{frame}

\begin{frame}
\frametitle{Feature relevance analysis results}
\begin{columns}
\column{0.3\textwidth}
\begin{figure}
	\caption{Negative relevance-only mean per feature group with the most relevant feature (V/UV) excluded.}
\end{figure}
\column{0.55\textwidth}
\begin{center}
  \includegraphics[width=\textwidth]{res/feature_relevance_ranking_-_mean_(negative_only_sum)_-_general_feature_categories_-_no_vuv}
\end{center}
\end{columns}
\end{frame}

\subsection{Contribution}



\section{Conclusions}

%7. Aspekty przyszlosciowe i praktyczne

\begin{frame}
\begin{center}
Thank you.
\end{center}
\end{frame}






















%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Additional slides
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{frame}
\frametitle{Intonation models}
\includegraphics[width=\textwidth]{res/inotnation_models_venn.png}
\end{frame}

\begin{frame}
\frametitle{Speech synthesis - Wavenet}
\begin{figure}
\begin{center}
  \includegraphics[width=\textwidth]{res/dilated_convolutions.png}
\end{center}
	\caption{Dilated causal convolutions. (Adopted from the original WaveNet paper).}
\end{figure}
The causality is expressed through the joint probability of the modeled waveform 
$\vec{x} = \{ x_1, \dots, x_T \}$
being factorized as a product of conditional probabilities of all previous timesteps (van den Oord 2016), i.e.:
\begin{equation}
p\left(\vec{x}\right) = \prod_{t=1}^{T} p\left(x_t \mid x_1, \dots ,x_{t-1}\right)
\end{equation}
\end{frame}

\begin{frame}
\frametitle{Speech synthesis - Wavenet}
\begin{columns}
\column{0.6\textwidth}
\begin{figure}
\begin{center}
  \includegraphics[width=\textwidth]{res/residual_blocks.png}
\end{center}
	\caption{Residual and skip connections from a stack of k gated convolutional layers (Adopted from the original WaveNet paper).}
\end{figure}
\column{0.4\textwidth}
\begin{exampleblock}{}
\tiny{Gated convolutional layers:
\begin{equation}
\vec{z} = \tanh \left(W_{f, k} \ast \vec{x}\right) \odot \sigma \left(W_{g, k} \ast \vec{x} \right), \label{eq:gated_activation}
\end{equation}
where $\ast$ denotes a convolution operator, $\odot$ denotes an element-wise multiplication operator, $\sigma(\cdot)$ is a sigmoid function, $k$ is the layer index, $f$ and $g$ denote filter and gate, respectively, and $W$ is a learnable convolution filter.}
\end{exampleblock}
\end{columns}
\end{frame}

\begin{frame}
\frametitle{Dataset}
\begin{figure}
\begin{center}
  \includegraphics[width=0.7\textwidth]{res/dataset_structure}
\end{center}
	\caption{Speech corpus built originally for the purpose of the Polish BOSS unit selection synthesizer (Demenko, Bachan, M{\"o}bius 2008; Demenko, Klessa, Szyma\'nski, Breuer, Hess 2010).}
\end{figure}
\end{frame}

\begin{frame}
\begin{figure}
\begin{center}
  \includegraphics[width=0.7\textwidth]{res/stress_accent_labels}
\end{center}
	\caption{Stress and accent labels used in the original Polish BOSS speech corpus.}
\end{figure}
\end{frame}

\begin{frame}
\begin{figure}
\begin{center}
  \includegraphics[width=0.7\textwidth]{res/boundary_tones}
\end{center}
	\caption{Prosodic phrase boundary labels used in the original Polish BOSS speech corpus.}
\end{figure}
\end{frame}

\begin{frame}
\begin{figure}
\begin{center}
  \includegraphics[width=0.4\textwidth]{res/tonal_accents}
\end{center}
	\caption{Acoustic realizations of the 9 different accents. Adopted from (Demenko 1999).}
\end{figure}
\end{frame}

\begin{frame}
\frametitle{Dataset}
\begin{center}
  \includegraphics[width=\textwidth]{res/database_structure_explanation}
\end{center}
\end{frame}

\begin{frame}
\frametitle{Feature set}
\begin{columns}
\column{0.5\textwidth}
\begin{figure}
\begin{center}
  \includegraphics[width=0.6\textwidth]{res/segmental_features}
\end{center}
\caption{Segmental features for a quintphone-wide context.}
\end{figure}
\column{0.5\textwidth}
\begin{figure}
\begin{center}
  \includegraphics[width=0.8\textwidth]{res/suprasegmental_features}
\end{center}
\caption{Non-segmental features.}
\end{figure}
\end{columns}
\end{frame}

\begin{frame}
\includegraphics[width=0.4\textwidth]{res/network_architecture}
\end{frame}


\begin{frame}
\begin{figure}
\begin{center}
  \includegraphics[width=0.4\textwidth]{res/deep_taylor}
\end{center}
	\caption{Computational flow of deep Taylor decomposition. (Adopted from Montavon 2017).}
\end{figure}
\begin{block}{}
The relevance in this framework can be defined as:
\begin{equation}
R_j = \sum _k \frac{a_j w_{jk}}{\sum _{0,j} a_j w_{jk}} R_k
\end{equation}
\end{block}
\end{frame}


\end{document}
